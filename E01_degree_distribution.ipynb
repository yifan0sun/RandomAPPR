{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ac72119-4501-4456-b0cd-c6a0659a5235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "from ogb.linkproppred import PygLinkPropPredDataset\n",
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# Load the dataset\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    " \n",
    "import pickle\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "367a22f7-e277-4d37-9c57-1929f1404a63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filenames = [ 'Amazon0302.txt', 'web-Google.txt', 'facebook_clean_data/artist_edges.csv','as-skitter.txt','com-dblp.ungraph.txt','com-youtube.ungraph.txt','com-orkut.ungraph.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1295a193-2961-4093-bbf9-a619e562d516",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing top 10 lines of 'Amazon0302.txt':\n",
      "# Directed graph (each unordered pair of nodes is saved once): Amazon0302.txt\n",
      "# Amazon product co-purchaisng network from March 02 2003\n",
      "# Nodes: 262111 Edges: 1234877\n",
      "# FromNodeId\tToNodeId\n",
      "0\t1\n",
      "0\t2\n",
      "0\t3\n",
      "0\t4\n",
      "0\t5\n",
      "1\t0\n",
      "Printing top 10 lines of 'web-Google.txt':\n",
      "# Directed graph (each unordered pair of nodes is saved once): web-Google.txt\n",
      "# Webgraph from the Google programming contest, 2002\n",
      "# Nodes: 875713 Edges: 5105039\n",
      "# FromNodeId\tToNodeId\n",
      "0\t11342\n",
      "0\t824020\n",
      "0\t867923\n",
      "0\t891835\n",
      "11342\t0\n",
      "11342\t27469\n",
      "Printing top 10 lines of 'facebook_clean_data/artist_edges.csv':\n",
      "node_1,node_2\n",
      "0,1794\n",
      "0,3102\n",
      "0,16645\n",
      "0,23490\n",
      "0,42128\n",
      "0,3822\n",
      "0,9555\n",
      "0,18602\n",
      "0,14473\n",
      "Printing top 10 lines of 'as-skitter.txt':\n",
      "# Undirected graph: as-skitter.txt\n",
      "# Autonomous Systems (From traceroutes run daility in 2005 by skitter - http://www.caida.org/tools/measurement/skitter)\n",
      "# Note: There were 22622 nodes with degree 0\n",
      "# Nodes: 1696415 Edges: 11095298\n",
      "# FromNodeId\tToNodeId\n",
      "0\t1\n",
      "0\t2\n",
      "0\t3\n",
      "0\t4\n",
      "0\t5\n",
      "Printing top 10 lines of 'com-dblp.ungraph.txt':\n",
      "# Undirected graph: ../../data/output/dblp.ungraph.txt\n",
      "# DBLP\n",
      "# Nodes: 317080 Edges: 1049866\n",
      "# FromNodeId\tToNodeId\n",
      "0\t1\n",
      "0\t2\n",
      "0\t4519\n",
      "0\t23073\n",
      "0\t33043\n",
      "0\t33971\n",
      "Printing top 10 lines of 'com-youtube.ungraph.txt':\n",
      "# Undirected graph: ../../data/output/youtube.ungraph.txt\n",
      "# Youtube\n",
      "# Nodes: 1134890 Edges: 2987624\n",
      "# FromNodeId\tToNodeId\n",
      "1\t2\n",
      "1\t3\n",
      "1\t4\n",
      "1\t5\n",
      "1\t6\n",
      "1\t7\n",
      "Printing top 10 lines of 'com-orkut.ungraph.txt':\n",
      "# Undirected graph: ../../data/output/orkut.txt\n",
      "# Orkut\n",
      "# Nodes: 3072441 Edges: 117185083\n",
      "# FromNodeId\tToNodeId\n",
      "1\t2\n",
      "1\t3\n",
      "1\t4\n",
      "1\t5\n",
      "1\t6\n",
      "1\t7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def print_top_lines(file_path, num_lines=10):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for i, line in enumerate(file):\n",
    "                print(line.strip())\n",
    "                if i + 1 >= num_lines:\n",
    "                    break\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{file_path}' not found.\")\n",
    "for filename in filenames:\n",
    "\n",
    "    file_path = filename # Update with the path to your file\n",
    "    print(f\"Printing top 10 lines of '{file_path}':\")\n",
    "    print_top_lines(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e788f5b6-71fb-418d-a43f-e3969956ae06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def import_graph_from_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Skip header lines\n",
    "            for line in file:\n",
    "                if line.startswith('#'):\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # Read graph data\n",
    "            graph = nx.read_edgelist(file, nodetype=int, data=False)\n",
    "            return graph\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{file_path}' not found.\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "def import_graph_from_csv(file_path):\n",
    "    try:\n",
    "        # Read CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Create a new directed graph\n",
    "        graph = nx.DiGraph()\n",
    "        \n",
    "        # Add edges from DataFrame to the graph\n",
    "        for _, row in df.iterrows():\n",
    "            node_1 = row['node_1']\n",
    "            node_2 = row['node_2']\n",
    "            graph.add_edge(node_1, node_2)\n",
    "        \n",
    "        return graph\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{file_path}' not found.\")\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52745d-3cfd-4d9b-b12c-2361ec431c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing graph from 'Amazon0302.txt':\n",
      "dict_keys(['name', 'degrees', 'degree_dist', 'num_nodes', 'num_edges'])\n",
      "Number of nodes: 262111\n",
      "Number of edges: 899792\n",
      "6.86573245685988 420 262111 0.0016023745665004522\n",
      "Amazon0302\n",
      "Importing graph from 'web-Google.txt':\n",
      "dict_keys(['name', 'degrees', 'degree_dist', 'num_nodes', 'num_edges'])\n",
      "Number of nodes: 875713\n",
      "Number of edges: 4322051\n",
      "9.870930316210904 6332 875713 0.007230679457767556\n",
      "web-Google\n",
      "Importing graph from 'facebook_clean_data/artist_edges.csv':\n"
     ]
    }
   ],
   "source": [
    "names = []\n",
    "graphs = []\n",
    "degree_dists = []\n",
    "for file_path in filenames:\n",
    "    name = file_path[:-4]\n",
    "    name = name.replace('/','_')\n",
    "\n",
    "    try:\n",
    "        asdf\n",
    "        degrees = pickle.load(open('dataset/%s_degree.pkl' % name,'rb'))\n",
    "        if file_path.endswith('.csv'):\n",
    "            graph = import_graph_from_csv(file_path)\n",
    "        else:\n",
    "            graph = import_graph_from_file(file_path)  \n",
    "    except:\n",
    "        # Example usage\n",
    "        print(f\"Importing graph from '{file_path}':\")\n",
    "\n",
    "        if file_path.endswith('.csv'):\n",
    "            graph = import_graph_from_csv(file_path)\n",
    "        else:\n",
    "            graph = import_graph_from_file(file_path)\n",
    "\n",
    "        #graphs.append(graph)\n",
    "\n",
    "        #degree_distribution = np.bincount(degrees)\n",
    "        #degree_dists.append(degree_distribution)\n",
    "\n",
    "        degrees = [degree for (node, degree) in graph.degree()]\n",
    "        degree_distr = np.bincount(degrees)\n",
    "        \n",
    "        outobj = {'name':name,'degrees': degrees,'degree_dist':degree_distr,'num_nodes':graph.number_of_nodes(),'num_edges':graph.number_of_edges()}\n",
    "        print(outobj.keys())\n",
    "        pickle.dump(outobj,open('dataset/%s_degree.pkl' % name,'wb'))\n",
    "    \n",
    "    print(\"Number of nodes:\", outobj['num_nodes'])\n",
    "    print(\"Number of edges:\", outobj['num_edges'])\n",
    "    degrees = outobj['degrees']\n",
    "    print(np.mean(degrees),max(degrees), len(degrees), max(degrees)/len(degrees))\n",
    "\n",
    "    degree_dists.append(outobj['degree_dist'])\n",
    "    names.append(name)\n",
    "    print(name)\n",
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a31a44-6fed-4860-9eed-f72915d9ef85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d716966-bfd9-4486-9535-417a4285cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "datasets_n = [\n",
    "    'ogbn-arxiv',\n",
    "    'ogbn-products',\n",
    "    'ogbn-proteins',\n",
    "    #'ogbn-mag'\n",
    "    'ogbn-papers100M'\n",
    "]\n",
    "\n",
    "datasets_l = [\n",
    "    'ogbl-citation2',\n",
    "    'ogbl-collab',\n",
    "    'ogbl-ppa'\n",
    "]\n",
    "\n",
    "#datasets_g = [\n",
    "#    'ogbg-code2',\n",
    "#    'ogbg-molhiv',\n",
    "#    'ogbg-molpcba',\n",
    "#]\n",
    "\n",
    "datasets = []\n",
    "for obj in datasets_n:\n",
    "    print(obj)\n",
    "    dataset = PygNodePropPredDataset(name=obj)\n",
    "    datasets.append((obj,dataset))\n",
    "for obj in datasets_l:\n",
    "    print(obj)\n",
    "    dataset = PygLinkPropPredDataset(name=obj)\n",
    "    datasets.append((obj,dataset))\n",
    "#for obj in datasets_g:\n",
    "#    print(obj)\n",
    "#    dataset = PygGraphPropPredDataset(name=obj)\n",
    "#    datasets.append((obj,dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b541685-345f-45db-a26e-bcaa3bf4e317",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the graph object\n",
    "\n",
    "for d in datasets:\n",
    "    name = d[0]\n",
    "    print(name)\n",
    "    graph = d[1][0]\n",
    "    print(graph)\n",
    "    \n",
    "    legendnames.append(name)\n",
    "\n",
    "    # Convert edge index to NetworkX graph\n",
    "    edge_index = graph.edge_index\n",
    "    \n",
    "    node_degree = [0 for k in range(graph.num_nodes)]\n",
    "    print(graph.edge_index.shape)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        node_degree = pickle.load(open('dataset/%s_degree.pkl' % name,'rb'))\n",
    "        print(\"Number of nodes:\", graph.number_of_nodes())\n",
    "        print(\"Number of edges:\", graph.number_of_edges())\n",
    "        name = name + (' (%d,%d)' %    ( graph.number_of_nodes(),graph.number_of_edges()))\n",
    "    except:\n",
    "        if edge_index.shape[1]  > 1000000*1000: continue\n",
    "        for i in range(edge_index.shape[1]):\n",
    "            if i % 1000000 == 0:\n",
    "                print(i,edge_index.shape[1])\n",
    "            e = edge_index[:,i]\n",
    "            node_degree[e[0]] += 1\n",
    "            node_degree[e[1]] += 1\n",
    "        print(node_degree[:10])\n",
    "\n",
    "\n",
    "        #G = nx.Graph()\n",
    "        #G.add_edges_from(edge_index.t().tolist())\n",
    "\n",
    "        #degrees = [degree for (node, degree) in nx.degree(G)]\n",
    "\n",
    "\n",
    "        #degree_distribution = np.bincount(degrees)\n",
    "        #degree_dists.append(degree_distribution)\n",
    "        #print(\"Number of nodes:\", G.number_of_nodes())\n",
    "        #print(\"Number of edges:\", G.number_of_edges())\n",
    "        pickle.dump(node_degree,open('dataset/%s_degree.pkl' % name,'wb'))\n",
    "    \n",
    "    max_degree = max(node_degree)\n",
    "    mean_degree = np.mean(node_degree)\n",
    "    print(mean_degree,max_degree, max_degree/len(node_degree), max_degree/mean_degree)\n",
    "    degree_dists.append(node_degree)\n",
    "    names.append(name)\n",
    "    \n",
    "sadf    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f88c4f31-4ef0-4e5b-bea5-7875da7c2aac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filenames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(filenames)):\n\u001b[0;32m      4\u001b[0m     degree_distribution \u001b[38;5;241m=\u001b[39m degree_dists[g]\n\u001b[0;32m      5\u001b[0m     graph \u001b[38;5;241m=\u001b[39m graphs[g]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filenames' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure(figsize=(4, 5))\n",
    "for g in range(len(filenames)):\n",
    "    \n",
    "    degree_distribution = degree_dists[g]\n",
    "    graph = graphs[g]\n",
    "    \n",
    "    n,e = graph.number_of_nodes(),graph.number_of_edges()\n",
    "    legendnames[g] += ' (%dK, %dK)' % (n/1000,e/1000)\n",
    "    \n",
    "    plt.subplot(2,1,1) \n",
    "    plt.plot(range(len(degree_distribution)), degree_distribution/np.sum(degree_distribution), label=legendnames[g])\n",
    "    plt.subplot(2,1,2) \n",
    "    plt.loglog(range(len(degree_distribution)), degree_distribution/np.sum(degree_distribution), label=legendnames[g])\n",
    "\n",
    "\n",
    "plt.ylim([0,1])\n",
    "plt.subplot(2,1,1) \n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Frequency\") \n",
    "plt.legend(fontsize=9)\n",
    "plt.subplot(2,1,2) \n",
    "plt.xlabel(\"Degree (log)\")\n",
    "plt.ylabel(\"Frequency (log)\") \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('degree_distr.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79c8a7fd-e9ba-4134-9cbc-5307d34ebeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "political-blog\n",
      "political-blog (1222,33431)\n",
      "27.357610474631752 351 0.2872340425531915 12.830067901049924\n",
      "citeseer\n",
      "citeseer (2110,7336)\n",
      "3.476777251184834 99 0.04691943127962085 28.47464558342421\n",
      "cora\n",
      "cora (2485,10138)\n",
      "4.0796780684104625 168 0.0676056338028169 41.179719865851254\n",
      "mnist-tr-nei10\n",
      "mnist-tr-nei10 (12000,194178)\n",
      "16.1815 226 0.018833333333333334 13.966566758335135\n",
      "pubmed\n",
      "pubmed (19717,88648)\n",
      "4.496018664096972 171 0.008672718973474667 38.03364994134103\n",
      "blogcatalog\n",
      "blogcatalog (10312,667966)\n",
      "64.7756012412723 3992 0.38712179984484096 61.628142749780686\n",
      "youtube\n",
      "youtube (1134890,5975248)\n",
      "5.265045951590022 28754 0.025336376212672596 5461.300863160826\n",
      "ogbn-arxiv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../graph_datasets/ogbn-arxiv.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m name \u001b[38;5;241m=\u001b[39m dataset\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(name)\n\u001b[1;32m---> 33\u001b[0m csr_mat, labels \u001b[38;5;241m=\u001b[39m load_graph_data(dataset)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     node_degree \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_degree.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m name,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[1;32mIn[27], line 16\u001b[0m, in \u001b[0;36mload_graph_data\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m     exit(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m f \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../graph_datasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m csr_mat \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(\n\u001b[0;32m     18\u001b[0m     (f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m], f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices\u001b[39m\u001b[38;5;124m\"\u001b[39m], f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindptr\u001b[39m\u001b[38;5;124m\"\u001b[39m]), shape\u001b[38;5;241m=\u001b[39mf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     19\u001b[0m labels \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../graph_datasets/ogbn-arxiv.npz'"
     ]
    }
   ],
   "source": [
    "\n",
    "def datasets():\n",
    "    dataset_list = ['political-blog', 'citeseer', 'cora', 'mnist-tr-nei10',\n",
    "                    'pubmed', 'blogcatalog', 'youtube', 'ogbn-arxiv']\n",
    "    #dataset_list = ['political-blog', 'citeseer', 'cora','pubmed','mnist-tr-nei10','blogcatalog']\n",
    "    #dataset_list = ['political-blog', 'citeseer', 'cora','pubmed','mnist-tr-nei10','blogcatalog']\n",
    "    weighted_dict = {'political-blog': False, 'citeseer': False, 'cora': False,\n",
    "                     'pubmed': False, 'mnist-tr-nei10': True, 'blogcatalog': False,\n",
    "                     'youtube': False, 'ogbn-arxiv': False}\n",
    "    return dataset_list, weighted_dict\n",
    "\n",
    "\n",
    "def load_graph_data(dataset='citeseer'):\n",
    "    if dataset not in datasets()[0]:\n",
    "        print(f\"{dataset} does not exist!\")\n",
    "        exit(0)\n",
    "    f = np.load(f'../../graph_datasets/{dataset}.npz')\n",
    "    csr_mat = sp.csr_matrix(\n",
    "        (f[\"data\"], f[\"indices\"], f[\"indptr\"]), shape=f[\"shape\"])\n",
    "    labels = f['labels']\n",
    "    return csr_mat, labels\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "dataset_list, _ = datasets()\n",
    "\n",
    "\n",
    "for dataset in dataset_list:\n",
    "    name = dataset\n",
    "    print(name)\n",
    "    \n",
    "    csr_mat, labels = load_graph_data(dataset)\n",
    "    try:\n",
    "        \n",
    "        node_degree = pickle.load(open('dataset/%s_degree.pkl' % name,'rb'))\n",
    "    except:\n",
    "        \n",
    "        \n",
    "    \n",
    "        A = csr_mat.copy()\n",
    "        A.data[:] = 1\n",
    "\n",
    "        # Calculate node degrees\n",
    "        node_degree = np.array(A.sum(axis=1)).flatten()\n",
    "    \n",
    "      \n",
    "        pickle.dump(node_degree,open('dataset/%s_degree.pkl' % name,'wb'))\n",
    "     \n",
    "    num_nonzero_elements = csr_mat.nnz\n",
    "    num_nodes, num_edges = csr_mat.shape[0], num_nonzero_elements\n",
    " \n",
    "    name = name + (' (%d,%d)' %    ( num_nodes,num_edges))\n",
    "    print(name)\n",
    "    max_degree = max(node_degree)\n",
    "    mean_degree = np.mean(node_degree)\n",
    "    print(mean_degree,max_degree, max_degree/len(node_degree), max_degree/mean_degree)\n",
    " \n",
    " \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af62826-62fb-4b35-895f-e465c0cf99c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
